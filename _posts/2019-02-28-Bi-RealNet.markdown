---
title: 'Bi-Real Net'
date: 2019-03-23
permalink: /posts/2019/03/quantization/
tags:
  - quantization
  - CNN
---

Bi-Real Net: Enhancing the Performance of 1bit CNNs with Improved Representational Capacity and Advanced Training Algorithm

# intro
motivation与其他网络压缩、量化的paper类似，无外乎对硬件的要求较高，需要较高的计算、存储资源。

作者将解决该问题的方法分为了三类：第一类是减少参数，即用更加高效的CNN架构、模块；第二类是压缩权重；第三类是同时压缩权重和激活。

本文中，作者主要研究第三种方法，即binary CNNs，也叫做1-bit CNNs。通过这种方法，作者就将计算复杂度高的矩阵乘法操作，转化为了XNOR和bit-count操作。但是这种方法在小尺寸的数据集(CIFAR-10和MNIST)上取得了可比较的结果，但是在大尺寸(large-scale)的数据集(ImageNet)上会出现较大的性能损失。**作者认为这揭示了1-bit CNNs的容量(capability，这篇paper主要思路也就是围绕着提高网络的容量展开的)不足。**

作者认为1-bit CNNs还有提升的空间，主要是基于一个重要的观察(划重点，好论文来自于对现有工作的深入观察，以及痛点的发现、分析、解决)——激活在通过BN层之后，会变得更接近真实值(即取值范围更大了)，但之后又会被sign函数量化到-1和+1。显然，那些取值范围更大的、或者说真实的激活，具有更多的信息，但是在原论文，即BNN中却被丢掉了。

作者为了解决这个问题，提出通过一个简单的捷径连接(shortcut,就是一个skip连接)，以保持这些真实激活，并称其为Bi-Real Net(翻译无力，双重真实？？？英语真是太有意思了)。

此外，作者还提出了一个新的，用来训练1-bit CNNs的训练算法，主要包括3个创新点：

- 用来量化激活的sign函数是不可微的，作者提出用一个分段的线性函数来近似其导数，是由它的二阶近似函数(多项式函数)导出的；
- 由于关于二值化的权重的损失的梯度通常较小，难以改变其符号(+1与-1)，作者提出对权重的梯度使用量级感知的梯度(Magnitude-aware)，通过改进的梯度，可以更有效地更新权重；
- 提出在预训练时使用clip函数替代ReLU函数，以更好的初始化。

# Methodology

方法介绍分为三个部分，首先分析了标准CNN与其表示容量(Representational Capability)，然后分析了所提出的Bi-Real net的具体结构与其表示容量，最后介绍了训练Bi-Real net的具体方法。

## standard 1-bit CNNs

首先要注意一点，在1-bit CNNs中，weight和activation的取值都是{-1，+1}，因此一个1x3x3的卷积核，一次卷积的结果为{-9,-7,...,9}；一个32x3x3的卷积核，一次卷积的结果为{-288,-286,...,+288}。

1-bit CNNs最大的好处就在于之前计算复杂度高的矩阵乘法被大大的简化了，原理见下图：

![avatar](https://github.com/pppLang/pppLang.github.io/blob/master/_posts/Bi-RealNet/fig2.PNG)

作者定义了网络的表达容量——$\R(x)$。标准的1-bit CNN表达容量如图所示：

![avatar](https://github.com/pppLang/pppLang.github.io/blob/master/_posts/Bi-RealNet/fig2.PNG)

## Bi-Real Net

文章所提的shortcut结构即加了一个skip连接，模型的原始模块包括三部分:sign->conv->bn，所加连接就是把每个模块的输入加到了bn后，就如上图(b)所示。

这样，原本每个模块输入的$A_r^l$的容量(即可取值的个数)为289，而BN层输出的$A_r^l+1$取值容量同样为289，但是两者的**取值范围是不同的**，经过BN层之后，会将激活放缩、平移到不同的区间，这样两者相加的取值容量就变为$289^2$。

(此处有疑问，按这样的话，下一模块的输入的容量就是$289^2$，但这样的话，之后的容量就会迭代的不停增加，但一个问题就是这些数据是多少位的？要彻底明白还需要之后再看代码。这就涉及到了1-bit CNN的定义，可能仅仅是指进入卷积的激活与其权重是1-bit)

## training Bi-Real Net

由于1-bit网络的激活与权重都是1-bit的，传统的SGD方法很难直接用于1-bit网络的训练。主要存在两点问题：

- 1-bit网络中引入的sign函数不可微；
- 直接对二值化的激活计算loss，并求梯度，所得梯度往往较小，不足以改变二值化的权重的符号。

在BNN中，针对q1，作者用分段线性函数的梯度来近似sign函数的梯度；针对q2，作者用由二值化权重计算得到的二值化激活的损失的梯度，来更新真实的权重，再根据真实权重的符号来更新二值化权重。

本文作者主要针对BNN的方法，提出了三点优化：

1. 对sign函数的导数的更好近似：用一个分段二次函数来近似sign函数，从画的图中，很明显可以看到，是更好的近似。
2. 二值化权重的量级感知梯度：首先明确，二值化权重由真实权重输入sign函数得到，优化过程中，不断更新真实权重，进而更新二值化权重。通过链式法则化简，最终可以发现真实权重的梯度，只与其符号相关，而与其量级无关。因此，作者改进了由真实权重获取二值化权重的函数，使得真是权重的梯度不仅与其符号相关，还与其量级相关。
3. 初始化：之前的1-bit网络的二值化的权重的初始化，是直接由真实权重取sign得到的。但是二值化之后，激活变成了{-1,+1}，但是之前通过ReLU生成的激活是非负的，两者相差较大，初始化的效果也不会太好。作者提出，在进行二值化之前，先将ReLU替换成clip(-1,x,+1)，与之后相符，可以获得更好的初始化。

# experiments

之前好多文章的实验都没好好看，因为觉得不太重要。但最近发现，除了idea，实验的设计也很重要，除了对比外，还要充分证明提出方法的有效性(消融实验，ablation study)，要让别人挑不出毛病，这次的要好好看一下。

看了一下，发现也没写啥。注意两点：

- 通过XNOR和bit-count操作实现的加速效果，本文并没有进行实际测试，其FLOPs仅仅是用真实浮点数计算的数据乘以了1/64。？？？这也可以？？？
- 与XNOR-Net相同，实际网络中，都保留了第一层卷积层最后一层全连接层的真实值。以及保留了resnet中的skip连接的真实值。
